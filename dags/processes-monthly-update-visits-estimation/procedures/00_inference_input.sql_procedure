CREATE OR REPLACE PROCEDURE
  `storage-prod-olvin-com.procedures.inference_input`(events_table STRING,
    poi_table STRING,
    metadata_table STRING,
    grouping_table STRING,
    gcs_path STRING)
WITH CONNECTION `storage-prod-olvin-com.eu.prod_spark_connection` OPTIONS (engine='SPARK',
    properties=[])
  LANGUAGE python AS R"""
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
import os,json
events_tbl = str(json.loads(os.environ["BIGQUERY_PROC_PARAM.events_table"]))
poi_tbl = str(json.loads(os.environ["BIGQUERY_PROC_PARAM.poi_table"]))
metadata_tbl = str(json.loads(os.environ["BIGQUERY_PROC_PARAM.metadata_table"]))
grouping_tbl = str(json.loads(os.environ["BIGQUERY_PROC_PARAM.grouping_table"]))
gcs_path = str(json.loads(os.environ["BIGQUERY_PROC_PARAM.gcs_path"]))


spark = SparkSession.builder.appName("spark-read-test").getOrCreate()

# Load data from BigQuery.
events = spark.read.format("bigquery") \
  .option("table", events_tbl) \
  .load()
poi = spark.read.format("bigquery") \
  .option("table", poi_tbl) \
  .load()
metadata = spark.read.format("bigquery") \
  .option("table", metadata_tbl) \
  .load()
dict = spark.read.format("bigquery") \
  .option("table", grouping_tbl) \
  .load()

joined_events = events.join(dict, on="fk_sgplaces", how="inner")
joined_poi = poi.join(dict, on="fk_sgplaces", how="inner")
joined_metadata = metadata.join(dict, on="fk_sgplaces", how="inner")

joined_events.repartition("group_id").write.mode('overwrite').partitionBy("group_id").option("overwriteSchema", "true").parquet(gcs_path+"/events")

joined_poi.repartition("group_id").write.mode('overwrite').partitionBy("group_id").option("overwriteSchema", "true").parquet(gcs_path+"/poi")

joined_metadata.repartition("group_id").write.mode('overwrite').partitionBy("group_id").option("overwriteSchema", "true").parquet(gcs_path+"/metadata")

""";